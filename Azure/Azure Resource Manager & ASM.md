<blockquote><p><em>Выражаем благодарность за подготовку статьи Михаилу Тряхову (<a href="https://habrahabr.ru/users/PerseptronYar/">@PerseptronYar</a>) из компании Akvelon (Ярославль) за помощь в написании данной статьи. Михаил работает в команде разработчиков Microsoft Azure CLI (Command Line Interface) со специализацией на Networking Services. </em></p></blockquote>

Всем привет! 

C 2008 года те из нас, кто работал с Azure, возможно, сами того не зная, использовали так называемый режим ASM (Azure Service Manager), который теперь называется классическим. Платформа Azure меж тем росла, развивалась, породила множество полезных сервисов, поддержку новых платформ и много еще чего хорошего. С ростом поддерживаемых и все более усложняющихся архитектур компонентов назрел целый ряд изменений, который решено было вынести в отдельный комплекс — ARM (Azure Resource Manager). В данной статье я поделюсь некоторыми моментами, с которыми сталкиваешься, осваивая данную платформу. Но сначала вспомним, что же такого серьезно нового оно нам несет. 

![](https://acom.azurecomcdn.net/80C57D/cdn/mediahandler/docarticles/dpsmedia-prod/azure.microsoft.com/en-us/documentation/articles/virtual-networks-create-vnet-arm-pportal/20160204105517/includes/virtual-networks-create-vnet-arm-pportal-include/vnet-create-arm-pportal-figure1.gif)

<habracut>
Первое, что бросится в глаза менее хардкорным читателям, которые, как и раньше, используют [веб-версию портала](https:\\manage.windowsazure.com) управления Azure, это сам портал, измененный как по форме, так и по содержанию.  [**Новая версия**](https://portal.azure.com/) в данный момент интенсивно дорабатывается и уже сейчас позволяет выполнять удобным образом большинство необходимых действий. Что касается usability — не бывает коренной смены структуры сайта, которая бы сразу резко обрадовала пользователей. Некоторые сложности при выполнении привычных действий обязательно возникнут. Посему поддержка обеих версий сайта в ближайшей перспективе — шаг со стороны Microsoft логичный и весьма лояльный.

Перемены наступают и в Azure Service Management API. Прежние порталы \*.windowsazure.com работают через API, свойственный классическому режиму, новые же, ARM-овские, порталы \*.azure.com используют уже новый API, на деталях которого мы и сосредоточимся. Вводится понятие *типа ресурса (resource type)* и соответствующего ему API. Виртаульной машине - своё, базе данных - своё. Детальнее новый REST API мы посмотрим чуть позже - пока же коснемся чуть более существенных различий. 

Ведь следующее нововведение бьёт по очень обременительному недостатку в работе с классическим API. Прежде мы радостно могли создать все, что нашей душе угодно, от blob-a до web-сайта. Но боль начиналась уже на следующем шаге - этапе работы с безопасностью, ибо каждый элемент конфигурировался отдельно. Не было возможности, например, единообразно указать доступ к скопу виртуалок, или установить схожие порядки в доступе к web-сайту и его сервисам, базе данных. Работа с ролями тоже какая-то кастрированная - у подписки есть админ и со-админы. Всё! Никакой security matrix, распределения ролей. Такой вот многолетний танец вокруг одного и того же столба. 
ARM позволяет создавать *группы ресурсов (resource groups)*, в рамках которых создавать нужные совокупности приложений, сервисов и баз данных. Теперь имеет место быть трехступенчатая иерархия - подписка (как корень всего), группа ресурсов и ресурс. Все они используют модель авторизации *RBAC (role based access control)*, которые имеют долгожданный скоп ролей (включая owner, contributor и др.) с соответствующими правами доступа (Access Control List, пример посмотрим ниже). 

![](https://msandbu.files.wordpress.com/2015/05/image11.png)

Ресурсы могут (и, по замыслу создателей, должны) быть помечены тегами. Созданные ресурсы могут распределяться по группам и тегам согласно функционалу, который они несут, фильтроваться по использующим их продуктам, ответственным сотрудникам и департаментам компании... Это позволяет еще более эффективно группировать и систематизировать ресурсы, визуализировать распределение траффика и проводить анализ производимых затрат. Это позволяет более очевидным образом оптимизировать расходы на поддержание работы развернутой архитектуры и хоть где-то уже, наконец, сэкономить.

Существует целый ряд ситуаций, когда сгруппированность нескольких ресурсов упрощает их единообразную модификацию. Самое простое, как водится, все удалить. Но более конструктивными кейсами являются настройка политики безопасности, конфигурирование ip-адресов, балансировка и, как мы увидим в этой и последующих статьях, многое другое. На практике средства Azure в ASM и ARM режимах используются вполне схожим образом и расходятся лишь в незначительных деталях. Переучиваться и долго, с осторожностью мигрировать все свои многолетние наработки, благо, не придется. Уже существуют стандартизированные гибридные решения для безболезненного и быстрого объединения уже разработанных, классических, и on-premises решений - об этом уже скоро на хабре.
Для начала же поговорим об использовании Azure Networking Services. Как уже рассказывалось в других статьях, Networking Services позволяют создавать свои виртуальные сети, в которых пользователь управляет местоположением (location) сети, безопасностью (security groups), подсетями (subnets), конфигурациями (NIC, VMs), удобно масштабировать... и, и, и.

Сразу пример. Рассмотрим простую и понятную архитектуру многоуровневого приложения на следующей схеме.

![MultiTier Application](http://i.imgur.com/43SXqsm.png)

В данном случае у нас имеется открытый и доступный пользователям Frontend Tier, бизнес-логика приложения в Application Tier и старый-добрый Backend Tier, который мы не забываем часто и корректно бэкапить. 
Сразу отмечу, для того, чтобы произвести те или иные действия в рамках подписки, существует целый букет возможностей.  Создание, модификация и удаление средств и сервисов Azure сейчас поистине широк. Помимо привычного [**web-портала**](https:\\manage.windowsazure.com), [**Azure PowerShell**](https://azure.microsoft.com/ru-ru/documentation/articles/powershell-install-configure/), [**REST API**](https://msdn.microsoft.com/en-us/library/azure/dn776326.aspx), вы также можете использовать активно дорабатываемая на момент написания статьи платформа [**Azure CLI**](https://azure.microsoft.com/ru-ru/documentation/articles/xplat-cli-install/). Реализуется она на Node.js проект с открытым кодом, что, как Вы наверняка понимаете, дает возможность полноценно использовать на Windows, Mac и многих дистрибутивах Linux.

Из тех материалов, что я изучил при подготовке к написанию данной статьи, я понял, что наиболее наглядным считается вариант с использованием REST API. Сводится он к созданию шаблона (template) в JSON-формате, содержащего всю необходимую информацию. Естественно, уже существует целая [**галерея подобных шаблонов **](https://azure.microsoft.com/en-us/documentation/templates) для развертывания подобной архитектуры. Выбранный вариант, наиболее подходящий под желания пользователя, кастомизируется и средствами, к примеру, Visual Studio развертывается в вашей подписке.

Нетрудно догадаться, что и платформы PowerShell, Azure CLI делают ни что иное, как формируют те же самые JSON-ы, но с плюшками в виде подсказок, валидаций, ограничений, которые позволят развернуть желаемое с наименьшей болью для нашего брата.

Прошу обратить внимание, что я сознательно стараюсь избежать обилия конкретных примеров кода, ибо это существенно перегрузит чтение, а статью обречет на устаревание и потерю актуальности. Давайте верить, что ссылки на мануалы и документацию будут поддерживаться куда лучше, чем примеры, обновляемые под контролем Вашего покорного слуги.

Собственно, пример рассматриваемой в нашем примере архитектуры уже реализован и опубликован. Оценить и скачать его можно [**здесь**](https://github.com/Azure/azure-quickstart-templates/tree/master/201-nsg-dmz-in-vnet). Выглядят JSON-ы, на первый взгляд, страшновато и избыточно. Но, когда шок спадет и разум возобладает, мы увидим, в сколь понятном и, оказывается, очевидном формате задаются все настройки виртуальной сети и ее элементов. При этом сразу же имеется возможность указать все необходимые зависимости, политику безопасности, разделение бизнес-логики.

Итак, для реализации подобной схемы средствами Azure, необходимо создать виртуальную сеть, в которой для каждого из описанных уровней создаются подсети. Следующим шагом будет настройка прав доступа. Для этих целей мы обратимся к Network Security Groups. NSG обеспечивают в сети доступ (или его отсутствие) к конкретным виртуальным машинам или целым подсетям. Они содержат набор правил ACL (тот самый Access Control List) для разрешения или отказа в доступе к указанной области. По умолчанию создается целый ряд inbound/outbound правил, которые позволяют покрыть существенный кусок области применения security groups. Azure предоставляет средства для указания всех необходимых настроек безопасности на каждом уровне. В частности, в приведенном мною шаблоне мы можем выделить, как с помощью задания правила ограничивается доступ к уровню данных – прямо и четко написано **"access": "denied"**. Задание правил производится достаточно очевидным образом, и я предлагаю изучить некоторые способы их кастомизации хотя бы на нашем примере самостоятельно.

<b>Network security groups</b> - отличная вещь и, помимо вышесказанного, включает в себя и удобные логгеры разных типов, что позволяет быстро и безболезненно оценить возникшие проблемы при использовании созданной архитектуры, изловить и утопить закравшуюся неточность. Пользователь может всесторонне проанализировать статистику обращений к данной группе. Все это обнадеживает. Некоторый ряд действий записывается по умолчанию, но есть возможность включить "дополнительное" логгирование в рамках данной Network Security Group - откроется куда более обширный спектр сохраняемой информации. Подключенные логи позволят в деталях посмотреть все необходимые данные по обращениям к связанным с NSG ресурсам. Это может сильно помочь в отладке, особенно на первых этапах формирования архитектуры. Как эти радости заполучить, что сказать и куда посмотреть - описано в документации [**Log analytics for NSGs**](https://azure.microsoft.com/en-us/documentation/articles/virtual-network-nsg-manage-log/).

![Analyze logs using Power BI](https://powerbi.microsoft.com/mediahandler/blog/legacymedia/7268.dashboard3.png/)

Для обеспечения доступа интернет-пользователей к «верхнему», frontend уровню, создается Application Gateway со всеми параметрами, как frontend ip, backend address pools, зависимости и многое другое. Почитать про настройку Application Gateways рекомендую дополнительно, например в [Application Gateways Docs](https://azure.microsoft.com/en-us/documentation/articles/application-gateway-introduction/), или, хотя бы, вдумчиво просмотреть представленный шаблон. Подробное описание настройки шлюза, чудес Load Balancer и Traffic Manager, Route Tables я рассказывать в этот раз не возьмусь - подробно об этом во следующей части.

Отмечу, что, раз уж мы заговорили о развертывании напрямую через REST API, при необходимости внести модификации в архитектуру – изменить какие-либо параметры, или добавить новые элементы (сеть, подсеть, да что угодно). Вам достаточно внести изменения в уже использованный шаблон. Это не значит, что вся с любовью расписанная махина будет повторно развернута и съест отдельную порцию ресурсов. Система умна и будет отслеживать лишь измененные куски кода и произведет любое [CRUD](https://ru.wikipedia.org/wiki/CRUD) действие оптимальным образом.

Итак, первый шаг сделан. Доработки, внедренные в ARM, выглядят вполне логичным и нужным продолжением предыдущих версий, и откровенных косяков пока не видно. [**Новый web-портал**](https://portal.azure.com/) достаточно быстро перестал пугать, а решения по модификации уже работающих решений выглядят вполне дружелюбно. Очень надеюсь на конструктивные отзывы и скорое продолжение. Спасибо вам!


<blockquote><p><em>Выражаем благодарность за подготовку статьи Михаилу Тряхову (<a href="https://habrahabr.ru/users/PerseptronYar/">@PerseptronYar</a>) из компании Akvelon (Ярославль) за помощь в написании данной статьи. Михаил работает в команде разработчиков Microsoft Azure CLI (Command Line Interface) со специализацией на Networking Services. </em></p></blockquote>

Приветствую Вас, дорогие читатели!

Продолжим описание основных средств разработки Microsoft Azure, начатый месяц назад в статье [о первых шагах](https://habrahabr.ru/company/microsoft/blog/278175/) в Azure Resource Manager (ARM). Мы успели поговорить об основных отличиях классического (Azure Source Management) подхода от нового режима ARM. Рассмотрели способы работы с JSON-шаблонами (templates), позволяющие более простым образом разворачивать и модифицировать архитектуру. При первой же возможности продемонстрировали способы настройки политики безопасности на примере трехуровневого приложения.

Я очень благодарю за фидбек, полученный пусть и в скромном объеме, но в конструктивном ключе. Я с огромным удовольствием построю данную статью в первую очередь на освещении областей, связанных с Вашими вопросами.

Напомню, мы рассматривали пример, схематически изображенной ниже
![](http://i.imgur.com/43SXqsm.png)
Для начала мне хотелось бы продолжить работу с Networking сервисами в ARM. Итак, давайте рассмотрим взаимодействие между внутренними уровнями приложения. 
![](http://i.imgur.com/Haut12B.png)
Напомню, мы уже прописали политику безопасности через Network Security Groups, запретив, в частности, доступ к сервисам баз данных (backend) напрямую из интернета.
{
	            "name": "Block_Internet",
	            "properties": {
	              "description": "Block Internet",
	              "protocol": "*",
	              "sourcePortRange": "*",
	              "destinationPortRange": "*",
	              "sourceAddressPrefix": "*",
	              "destinationAddressPrefix": "Internet",
	              "access": "Deny",
	              "priority": 200,
	              "direction": "Outbound"
	            }
	          }
Помимо этого мудрого поступка, предметная область может потребовать от нас и других ратных подвигов. Для того, чтобы обеспечить корректное взаимодействие между уровнями нашего приложения, нам необходимо сконфигурировать routing. Задача эта не самая частая, поскольку большинство необходимых кейсов прекрасно покрывается предоставляемыми по умолчанию System Routes.
Это позволяет не мучиться настройкой связей между виртуальными машинами в рамках virtual network, безотносительно подсетей (subnets), к которым они относятся. Системные маршруты, к тому же, обеспечивают обмен данных и вне созданной сети (в интернет, в другие сети через VPN). Автоматически создаются связи с таблицей маршрутов (route table). Для того, чтобы снизить размер нашего шаблона, предлагаю вынести данную задачу в несколько упрощенном виде.
![](http://i.imgur.com/nxwR2Mc.png)
Но мы здесь не для того, чтобы тратить много времени на данные по умолчанию сервисы. Рассмотрим менее тривиальные кейсы, когда нам не обойтись базовой системной маршрутизацией. Для таких целей может использоваться User Defined Routes (UDR). Это позволит нам создать маршруты, определенные пользователем и реализовать более сложный кастомный сценарий. К примеру, UDR поможет использовать виртуальные устройства в архитектуре Azure, обеспечить доступ к интернету через вашу локальную сеть. Настройка Firewall-a, анализа передаваемых по сети данных, детализации логгирования. 
Выглядеть это будет примерно следующим образом. Мы добавляем третью виртуалку, через которую мы и будем прогонять наш траффик для выполнения поставленных задач. 
![](http://i.imgur.com/d1dNXN4.png)
Предлагаю вновь обратиться к шаблону, доступному по [этой ссылке](https://github.com/MikhailTryakhov/azure-quickstart-templates/tree/master/201-userdefined-routes-appliance). Полагаю, если вы заглянете в него, то его объем даст понимание, почему я малодушно вынес этот таск из первоначальной схемы. Как и прежде, не пугаемся и продолжаем. В шаблоне мы, как и ранее, создаем виртуальную сеть и три подсети: frontend, backend и промежуточная между ними Virtual Appliance (subnet3). Frontend подсеть (subnet1) мы, помимо NSG, соотносим с таблицей маршрутов (route table), которая будет направлять исходящий трафик. Отмечу, что User Defined Routes пригодны для конфигурирования именно исходящего трафика, к тому же направление невозможно в рамках одной и той же подсети. 
В каждой подсети мы разворачиваем виртуальные машины и ставим им в соответствие Public IP адреса, конфигурируем network security groups, добавляя к дефолтным правилам своё, разрешая доступ по RDP. Ну и на десерт – добавляем упоминавшуюся выше route table, в которой прописываем правило, отражающее пункт назначение нашего маршрута (destination route).
{
  "type": "Microsoft.Network/routeTables",
  "name": "[variables('routeTableName')]",
  "apiVersion": "2015-05-01-preview",
  "location": "[parameters('location')]",
  "properties": {
    "routes": [
      {
        "name": "VirtualApplianceRouteToSubnet3",
        "properties": {
          "addressPrefix": "[parameters('subnet3Prefix')]",
          "nextHopType": "VirtualAppliance",
          "nextHopIpAddress": "[variables('NvmPrivateIPAddress')]"
        }
      }
    ]
  }
},
Я прошу обратить внимание на важный момент – мы указываем тип и адрес следущего получателя нашего трафика. В нашем случае мы указываем, что если траффик пришел в Subnet 3, то мы перенаправляем его на следующий (приватный) IP-адрес. 
Дополнительный шаг, который требуется, это установить необходимую связь между финальной Backend подсетью, ее приватным IP-адресом и нашей таблицей маршрутов. Для этого мы создаем каждой подсети Network Interface (NIC). Если для Frontend подсети все праздно и малоинтересно, то в конфигурации Backend-a все не так просто. В ней мы указываем связь публичного и приватного IP-адресов,  а также допускаем пересылку IP.
"properties": {
  "ipConfigurations": [
    {
      "name": "ipconfig1",
      "properties": {
        "privateIPAllocationMethod": "Static",
        "privateIPAddress": "[variables('NvmPrivateIPAddress')]",
        "publicIPAddress": {
          "id": "[resourceId('Microsoft.Network/publicIPAddresses', parameters('PublicIPNameForVM2'))]"
        },
        "subnet": {
          "id": "[variables('subnet2Ref')]"
        }
      }
    }
  ],
  "enableIPForwarding": true
}




